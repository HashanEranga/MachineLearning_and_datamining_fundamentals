{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Stopwords in NLP\n",
    "\n",
    "## Practice Stopwords\n",
    "\n",
    "To\n",
    "practice\n",
    "stopwords, we\n",
    "will\n",
    "generate\n",
    "a\n",
    "string\n",
    "variable\n",
    "called\n",
    "`corpus`\n",
    "containing\n",
    "several\n",
    "sentences.\n",
    "\n",
    "Here\n",
    "'s an example corpus:"
   ],
   "id": "a4f5f352544c6cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T09:23:23.663815Z",
     "start_time": "2025-12-10T09:23:22.797960Z"
    }
   },
   "cell_type": "code",
   "source": "from nltk.tokenize import sent_tokenize",
   "id": "8ae1dde61c4227a7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T09:23:31.083058Z",
     "start_time": "2025-12-10T09:23:30.997083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a string variable 'corpus' with multiple sentences\n",
    "corpus = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "This is an example of a sentence with common stopwords.\n",
    "Stopwords are words that we want to remove from our text data.\n",
    "For instance, 'is', 'and', 'the', etc., are common stopwords.\n",
    "Removing these stopwords can help us focus on more important words in our text.\n",
    "\"\"\"\n",
    "\n",
    "# Split the corpus into sentences\n",
    "sentences = sent_tokenize(corpus)\n",
    "print(sentences)"
   ],
   "id": "281e961d6b044ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n    The quick brown fox jumps over the lazy dog.', 'This is an example of a sentence with common stopwords.', 'Stopwords are words that we want to remove from our text data.', \"For instance, 'is', 'and', 'the', etc., are common stopwords.\", 'Removing these stopwords can help us focus on more important words in our text.']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "This code will create a string variable `corpus` and then use `nltk.tokenize.sent_tokenize()` to split the corpus into individual sentences. You can modify the `corpus` variable with your own text if needed.\n",
    "\n",
    "To see the list of stopwords in English, you can use:\n",
    "\n"
   ],
   "id": "3630f918155c09c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T09:24:44.463405Z",
     "start_time": "2025-12-10T09:24:41.589667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import the set of English stopwords from nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "print(english_stopwords)"
   ],
   "id": "ba8861c3ecb6fd9d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HashanEranga\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'don', 'these', 'other', 'are', 'won', 'here', 'we', 'whom', 'above', \"you'll\", 'was', 'needn', 'aren', \"it's\", \"he'll\", 'm', 'into', 'only', 'where', \"we'll\", 'didn', \"you've\", 'on', \"weren't\", \"isn't\", 'had', 'very', 'being', \"she's\", 'to', 'his', \"haven't\", 'd', \"it'll\", 'an', 'few', 'this', 'it', 'does', 'is', 'the', 'not', 'out', 'further', \"i'll\", 'ours', \"couldn't\", 'do', 'she', 'between', 'couldn', \"we've\", 'against', 'yours', 'yourselves', \"hadn't\", 'own', 'himself', 'mightn', 'i', 'what', \"should've\", 'will', \"doesn't\", 'haven', 'o', \"hasn't\", \"they're\", 'as', 'so', \"don't\", \"they'd\", \"i'd\", \"shouldn't\", 'through', 'both', 'theirs', 'then', 'than', 'mustn', 'yourself', 'again', 'me', 'there', 'some', 'he', 'my', 'they', \"he'd\", 'down', 'have', 'which', 'below', 'be', \"aren't\", 'been', 'our', 'in', 'ain', \"she'd\", \"won't\", 'll', 'that', 'from', 'any', 'and', 'those', 'why', 'can', 'off', 'who', 'under', \"mustn't\", 'same', 'hers', 'wouldn', 're', 'by', \"wasn't\", 'them', 've', 'your', 'ma', 'until', 'too', 'myself', 'because', \"they've\", \"wouldn't\", 'having', 'how', 'no', 'more', 'about', 'isn', 'wasn', 'while', 'y', 'if', 'of', \"they'll\", 's', \"that'll\", 'each', 'up', 'herself', \"it'd\", 'doing', 'ourselves', 'most', \"you're\", 'him', 'for', 'a', \"i've\", 'such', \"mightn't\", 'themselves', 'once', 'during', \"you'd\", 'were', 'should', 'am', 'now', \"she'll\", 'just', 'shan', 'did', 'after', 'its', 'over', 'their', 'doesn', \"we're\", 'nor', 'at', 'shouldn', 'weren', \"didn't\", \"shan't\", 'has', 'when', 'with', 'hasn', \"i'm\", \"we'd\", \"he's\", 'before', 'all', 't', \"needn't\", 'or', 'itself', 'hadn', 'her', 'but', 'you'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "This will print a set of common English stopwords. You can use this list to filter out stopwords from your text data during processing.\n",
    "\n",
    "Remember, you need the `nltk` package installed and the necessary datasets downloaded:\n",
    "\n"
   ],
   "id": "dbc94da548ac8392"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T09:24:58.651479Z",
     "start_time": "2025-12-10T09:24:58.576820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download the required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ],
   "id": "e25bd127a1a8bfd3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HashanEranga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HashanEranga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "This will ensure that you have access to the tokenizer and stopwords dataset.\n"
   ],
   "id": "38af76c6bc6575ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### using portStemmer",
   "id": "79c6dc50a97e920a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T09:46:26.270730Z",
     "start_time": "2025-12-10T09:46:26.267171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a string variable 'corpus' with multiple sentences\n",
    "corpus = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "This is an example of a sentence with common stopwords.\n",
    "Stopwords are words that we want to remove from our text data.\n",
    "For instance, 'is', 'and', 'the', etc., are common stopwords.\n",
    "Removing these stopwords can help us focus on more important words in our text.\n",
    "\"\"\"\n",
    "\n",
    "# Split the corpus into sentences\n",
    "sentences = sent_tokenize(corpus)\n",
    "print(sentences)"
   ],
   "id": "eecc820dac1fe2bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nThe quick brown fox jumps over the lazy dog.', 'This is an example of a sentence with common stopwords.', 'Stopwords are words that we want to remove from our text data.', \"For instance, 'is', 'and', 'the', etc., are common stopwords.\", 'Removing these stopwords can help us focus on more important words in our text.']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T09:46:42.345984Z",
     "start_time": "2025-12-10T09:46:42.340386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "english_stopwords = set(stopwords.words('english'))\n",
    "print(english_stopwords)"
   ],
   "id": "c1347d7c124096d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'don', 'these', 'other', 'are', 'won', 'here', 'we', 'whom', 'above', \"you'll\", 'was', 'needn', 'aren', \"it's\", \"he'll\", 'm', 'into', 'only', 'where', \"we'll\", 'didn', \"you've\", 'on', \"weren't\", \"isn't\", 'had', 'very', 'being', \"she's\", 'to', 'his', \"haven't\", 'd', \"it'll\", 'an', 'few', 'this', 'it', 'does', 'is', 'the', 'not', 'out', 'further', \"i'll\", 'ours', \"couldn't\", 'do', 'she', 'between', 'couldn', \"we've\", 'against', 'yours', 'yourselves', \"hadn't\", 'own', 'himself', 'mightn', 'i', 'what', \"should've\", 'will', \"doesn't\", 'haven', 'o', \"hasn't\", \"they're\", 'as', 'so', \"don't\", \"they'd\", \"i'd\", \"shouldn't\", 'through', 'both', 'theirs', 'then', 'than', 'mustn', 'yourself', 'again', 'me', 'there', 'some', 'he', 'my', 'they', \"he'd\", 'down', 'have', 'which', 'below', 'be', \"aren't\", 'been', 'our', 'in', 'ain', \"she'd\", \"won't\", 'll', 'that', 'from', 'any', 'and', 'those', 'why', 'can', 'off', 'who', 'under', \"mustn't\", 'same', 'hers', 'wouldn', 're', 'by', \"wasn't\", 'them', 've', 'your', 'ma', 'until', 'too', 'myself', 'because', \"they've\", \"wouldn't\", 'having', 'how', 'no', 'more', 'about', 'isn', 'wasn', 'while', 'y', 'if', 'of', \"they'll\", 's', \"that'll\", 'each', 'up', 'herself', \"it'd\", 'doing', 'ourselves', 'most', \"you're\", 'him', 'for', 'a', \"i've\", 'such', \"mightn't\", 'themselves', 'once', 'during', \"you'd\", 'were', 'should', 'am', 'now', \"she'll\", 'just', 'shan', 'did', 'after', 'its', 'over', 'their', 'doesn', \"we're\", 'nor', 'at', 'shouldn', 'weren', \"didn't\", \"shan't\", 'has', 'when', 'with', 'hasn', \"i'm\", \"we'd\", \"he's\", 'before', 'all', 't', \"needn't\", 'or', 'itself', 'hadn', 'her', 'but', 'you'}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:06:27.294259Z",
     "start_time": "2025-12-10T10:06:27.279766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Ensure the required NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize the stemmer and stopwords set\n",
    "stemmer = PorterStemmer()\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Example sentences (replace with your actual corpus)\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"This is an example of a sentence with common stopwords\",\n",
    "    \"Stopwords are words that we want to remove from our text data\"\n",
    "]\n",
    "\n",
    "# Process each sentence\n",
    "processed_sentences = []\n",
    "for sentence in sentences:\n",
    "    # Split sentence into words, stem them, and remove stopwords\n",
    "    words = sentence.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    filtered_words = [word for word in stemmed_words if word not in english_stopwords]\n",
    "    processed_sentence = ' '.join(filtered_words)\n",
    "    processed_sentences.append(processed_sentence)\n",
    "\n",
    "# Print the processed sentences\n",
    "for processed_sentence in processed_sentences:\n",
    "    print(processed_sentence)"
   ],
   "id": "cc4b5ddbdaa7c73c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick brown fox jump lazi dog\n",
      "thi exampl sentenc common stopword\n",
      "stopword word want remov text data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HashanEranga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HashanEranga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:26:51.542683Z",
     "start_time": "2025-12-10T10:26:50.675750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample paragraph\n",
    "paragraph = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. It is a well-known sentence used to demonstrate the features of a font, such as its typeface, size, and color.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the paragraph into sentences\n",
    "sentences = sent_tokenize(paragraph)\n",
    "\n",
    "# Initialize stemmer and stopwords list\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Process each sentence\n",
    "for sentence in sentences:\n",
    "    # Tokenize sentence into words\n",
    "    words = word_tokenize(sentence)\n",
    "\n",
    "    # Remove stopwords and stem the remaining words\n",
    "    stemmed_words = [stemmer.stem(word) for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Reconstruct the sentence with stemmed words\n",
    "    stemmed_sentence = ' '.join(stemmed_words)\n",
    "\n",
    "    print(stemmed_sentence)"
   ],
   "id": "e2925c419939b438",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick brown fox jump lazi dog .\n",
      "well-known sentenc use demonstr featur font , typefac , size , color .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HashanEranga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HashanEranga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:57:26.648916Z",
     "start_time": "2025-12-10T10:57:26.644227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus = \"\"\"\n",
    "inside a cell do the following create an example pharagraph for removing stop words. then use sent_tokenizer for make it in to sentense list after that use portStemmer on each sentence and remove the stop words use english stop words after that join the words into a sentence list\n",
    "\"\"\"\n",
    "\n",
    "sentences = sent_tokenize(corpus)"
   ],
   "id": "b4dab8fc0524e643",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:37:24.594934Z",
     "start_time": "2025-12-10T10:37:24.590013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "portStemmer = PorterStemmer()\n",
    "snowballStemmer = SnowballStemmer(\"english\")\n"
   ],
   "id": "46338f3406ebccfc",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:39:21.313159Z",
     "start_time": "2025-12-10T10:39:21.306002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_sentences = []\n",
    "for sentence in sentences:\n",
    "    # Split sentence into words, stem them, and remove stopwords\n",
    "    words = sentence.split()\n",
    "    # stemmed_words = [snowballStemmer.stem(word) for word in words if word not in english_stopwords]\n",
    "    stemmed_words = [portStemmer.stem(word) for word in words if word not in english_stopwords]\n",
    "    processed_sentence = ' '.join(stemmed_words)\n",
    "    processed_sentences.append(processed_sentence)\n",
    "\n",
    "# Print the processed sentences\n",
    "for processed_sentence in processed_sentences:\n",
    "    print(processed_sentence)"
   ],
   "id": "205bf01523d8dcb1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insid cell follow creat exampl pharagraph remov stop words.\n",
      "use sent_token make sentens list use portstemm sentenc remov stop word use english stop word join word sentenc list\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### using lemmar to remove stop words",
   "id": "2dfb4574ca47a5a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:57:14.718102Z",
     "start_time": "2025-12-10T10:57:08.001340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()"
   ],
   "id": "c2b00e68830dc502",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HashanEranga\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:57:48.308603Z",
     "start_time": "2025-12-10T10:57:47.017072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_sentences = []\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    stemmed_words = [lemmatizer.lemmatize(word, \"v\") for word in words if word not in english_stopwords]\n",
    "    processed_sentence = ' '.join(stemmed_words)\n",
    "    processed_sentences.append(processed_sentence)\n",
    "\n",
    "# Print the processed sentences\n",
    "for processed_sentence in processed_sentences:\n",
    "    print(processed_sentence)"
   ],
   "id": "8171493381785cdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside cell follow create example pharagraph remove stop words.\n",
      "use sent_tokenizer make sentense list use portStemmer sentence remove stop word use english stop word join word sentence list\n"
     ]
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
